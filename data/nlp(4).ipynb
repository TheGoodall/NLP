{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "752a2cfb-3d3a-466f-b42f-b45bbdb6aba6",
   "metadata": {},
   "source": [
    "### Install and import Stacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97d2a4f4-0c1e-444c-92e6-1eecf46c09cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy scipy numpy\n",
    "#!python -m spacy download en_core_web_lg\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "153e90c4-0f81-4889-9817-9b5952f07c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03becf06-20d8-41d8-a8dc-1c3ccc03ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44051f7c-7349-43a3-bbff-e80b79fc7ec4",
   "metadata": {},
   "source": [
    "### Import Data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "261d5308-2407-452b-8925-8f3dca261f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "import csv\n",
    "\n",
    "bodies: dict[int, str] = {}\n",
    "\n",
    "with open(\"work/fnc-1/train_bodies.csv\") as bodies_file:\n",
    "    bodies_reader = csv.reader(bodies_file)\n",
    "    \n",
    "    # Get rid of key from first entry in CSV file\n",
    "    _ = next(bodies_reader)\n",
    "             \n",
    "    for body in bodies_reader:\n",
    "        body_id = int(body[0])\n",
    "        body_str = body[1]\n",
    "        bodies[body_id] = body_str\n",
    "\n",
    "        \n",
    "        \n",
    "articles: list[tuple[str, str, str]] = []\n",
    "with open(\"work/fnc-1/train_stances.csv\") as stances_file:\n",
    "    stances_reader = csv.reader(stances_file)\n",
    "\n",
    "    # Get rid of key from first entry in CSV file\n",
    "    _ = next(stances_reader)\n",
    "    \n",
    "    for article in stances_reader:\n",
    "        articles.append((article[0], bodies[int(article[1])], article[2]))\n",
    "bodies = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b31933-3c14-49f0-ab89-99459974955b",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0468d5ba-2014-4bcd-a973-5ae8b2380457",
   "metadata": {},
   "source": [
    "### Tokenise text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1745a07d-d5d4-4526-8d1e-c1fc92ac5a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(articles):\n",
    "    import numpy\n",
    "    headlines = map(lambda article: article[0], articles)\n",
    "    bodies = map(lambda article: article[1], articles)\n",
    "    stances = map(lambda article: article[2], articles)\n",
    "\n",
    "    processed_articles = zip(\n",
    "            nlp.pipe(headlines, disable = ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']),\n",
    "            nlp.pipe(bodies, disable = ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']),\n",
    "            stances\n",
    "        )\n",
    "    return processed_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea17051f-da1b-4506-b4bb-48f8b8bee0f2",
   "metadata": {},
   "source": [
    "### Get Inverse document frequency for each Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "729eea57-8a91-429b-90f3-a0997e790e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99944it [00:54, 1820.08it/s]\n",
      "100% 29150/29150 [00:00<00:00, 712402.62it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_articles = tokenise(articles)\n",
    "token_freq = {}\n",
    "\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_docs = tqdm(chain.from_iterable(map(lambda a: (a[0], a[1]), processed_articles)))\n",
    "document_count = 0\n",
    "\n",
    "for doc in all_docs:\n",
    "    document_count += 1\n",
    "    already_added = set()\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.text not in already_added:\n",
    "            already_added.add(token.text)\n",
    "            try:\n",
    "                token_freq[token.text] += 1\n",
    "            except KeyError:\n",
    "                token_freq[token.text] = 1\n",
    "\n",
    "for word, count in tqdm(token_freq.items()):\n",
    "    token_freq[word] = np.log(document_count/count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de18319f-181e-4c57-8b55-bd1cc81688bf",
   "metadata": {},
   "source": [
    "### Give each term an ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58e94926-9052-4cf3-88fb-1f58649ba77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list(token_freq.keys())\n",
    "vector_size = len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3810273-c93e-4409-9416-ea94789da551",
   "metadata": {},
   "source": [
    "### Get vector pair for each headline - body pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c9f01331-f5c3-454a-af3f-ab2c84f2cdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "processed_articles = tokenise(articles)\n",
    "\n",
    "def calculate_td_idf(text):\n",
    "    vector = sparse.dok_array((vector_size, 1))\n",
    "    for token in text:\n",
    "        i = tokens.index(token.text)\n",
    "        value = token_freq[token.text]\n",
    "        vector[i, 0] += value\n",
    "\n",
    "    return vector\n",
    "def td_idf(processed_articles):\n",
    "    for article in processed_articles:\n",
    "        headline = calculate_td_idf(article[0])\n",
    "        body = calculate_td_idf(article[1])\n",
    "        stance = article[2]\n",
    "        yield headline, body, stance\n",
    "    \n",
    "    \n",
    "tf_idf_articles = td_idf(processed_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3dec33-46ca-4a1e-8667-b890228436b1",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d39fadf4-76f5-465d-9cb6-81e3498cff07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "171ff315-c9b2-450b-b23e-dbb8c246726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise_and_transform(articles):\n",
    "    import numpy\n",
    "    headlines = map(lambda article: article[0], articles)\n",
    "    bodies = map(lambda article: article[1], articles)\n",
    "    stances = map(lambda article: article[2], articles)\n",
    "\n",
    "    processed_articles = zip(\n",
    "            map(lambda x: model(**x), map(lambda x: tokenizer(x, return_tensors=\"pt\"), headlines)),\n",
    "            map(lambda x: model(**x), map(lambda x: tokenizer(x, return_tensors=\"pt\"), bodies)),\n",
    "            stances\n",
    "        )\n",
    "    return processed_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78943f77-d001-4960-8482-182633d1275c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_articles = tokenise_and_transform(articles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c01c90-bce2-41bc-815e-48aec48bbe9e",
   "metadata": {},
   "source": [
    "# Related/Unrelated Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3eef9b6a-399f-4c91-a9e6-38c216584882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 5000/5000 [02:05<00:00, 39.91it/s]\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import vstack\n",
    "import itertools\n",
    "x_idf = []\n",
    "y_idf = []\n",
    "l = 5000\n",
    "for article in tqdm(itertools.islice(tf_idf_articles, l), total=l):\n",
    "    x_idf.append(vstack((article[0], article[1])).toarray().squeeze(1))\n",
    "    if article[2] == \"unrelated\":\n",
    "        y_idf.append(0)\n",
    "    else:\n",
    "        y_idf.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1c35323e-4646-4429-9074-c213a10baeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 0/5000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 19 but got size 100 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m l \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m tqdm(itertools\u001b[38;5;241m.\u001b[39mislice(transformer_articles, l), total\u001b[38;5;241m=\u001b[39ml):\n\u001b[0;32m----> 7\u001b[0m     x_idf\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_hidden_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marticle\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_hidden_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m article[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munrelated\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      9\u001b[0m         y_idf\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 19 but got size 100 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "from torch import cat\n",
    "import itertools\n",
    "x_trf = []\n",
    "y_trf = []\n",
    "l = 5000\n",
    "for article in tqdm(itertools.islice(transformer_articles, l), total=l):\n",
    "    x_idf.append(cat((article[0].last_hidden_state, article[1].last_hidden_state)).squeeze(1))\n",
    "    if article[2] == \"unrelated\":\n",
    "        y_idf.append(0)\n",
    "    else:\n",
    "        y_idf.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1b25da-e985-43e3-940a-fdd80512e39c",
   "metadata": {},
   "source": [
    "### ML - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f0da4c10-9231-4d86-88a9-3cfc7e1d8391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:   17.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_jobs=12, verbose=True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import vstack\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "mltfidf = RandomForestClassifier(verbose=True, n_estimators=100, n_jobs=12)\n",
    "mltfidf.fit(x_idf,y_idf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2b80d7c3-0a8a-4b9a-a0ba-b945ed3fd117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 500/500 [00:13<00:00, 36.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 412/500 = 0.824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "test_x_idf = []\n",
    "test_y_idf = []\n",
    "l = 500\n",
    "for article in tqdm(itertools.islice(tf_idf_articles, l), total=l):\n",
    "    test_x_idf.append(vstack((article[0], article[1])).toarray().squeeze(1))\n",
    "    if article[2] == \"unrelated\":\n",
    "        test_y_idf.append(0)\n",
    "    else:\n",
    "        test_y_idf.append(1)\n",
    "    \n",
    "test_y_pred = mltfidf.predict(test_x_idf)\n",
    "count = len(test_y_pred)\n",
    "correct = sum(y == t for y,t in zip(test_y_pred, test_y_idf))\n",
    "print(f\"Accuracy = {correct}/{count} = {correct/count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c7a8cc-b78c-4f29-b43f-a5550caaf736",
   "metadata": {},
   "source": [
    "# ML - Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe478c4c-76ec-4696-92fd-e0d1e88ca5c0",
   "metadata": {},
   "source": [
    "### DL - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9e89ea-35e9-4b5f-a842-8bd78b5ca438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use RMSprop; the optim package contains many other\n",
    "# optimization algorithms. The first argument to the RMSprop constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "while 1:\n",
    "    \n",
    "    \n",
    "    y_pred = model(x)\n",
    "\n",
    "    loss = loss_fn(y_pred, y)\n",
    "\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c1d94b-a992-4d9a-8719-5d3b03038f7c",
   "metadata": {},
   "source": [
    "### DL - Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2def3085-8a14-492d-8d6b-0a8879691f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use RMSprop; the optim package contains many other\n",
    "# optimization algorithms. The first argument to the RMSprop constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "while 1:\n",
    "    \n",
    "    \n",
    "    y_pred = model(x)\n",
    "\n",
    "    loss = loss_fn(y_pred, y)\n",
    "\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
